{"cells":[{"cell_type":"markdown","source":["# CUSTOMER CHURN PREDICTION\n\n#####Churn prediction is one of the most popular Big Data use cases in business. It consists of detecting customers who are likely to cancel a subscription to a service. Although originally a telco giant thing, this concerns businesses of all sizes, including startups.\n\n![DLR](https://raw.githubusercontent.com/Krishnakali/Big-Data-Project/master/Churn.png)\n\nThe dataset contains 3,333 observations\nWe will use this information to predict if a customer is likely to discontinue using the given telecom service. The dataset is rather clean, and consists of both numeric and categorical variables.\n\n**Attribute Information:**\n* **state:** State Name\n* **account** length: Account length\n* **area code:** Area Code\n* **phone number:** Phone number\n* **international plan:** Whether they have international plan\n* **voice mail plan:** Whether they have voice mail plan\n* **number vmail messages:** No. of voice mail messages\n* **total day minutes:** numeric.\n* **total day calls:** numeric.\n* **total day charge:** numeric.\n* **total eve minutes:** numeric.\n* **total eve calls:** numeric.\n* **total eve charge:** numeric.\n* **total night minutes:** numeric.\n* **total night calls:** numeric.\n* **total night charge:** numeric.\n* **total intl minutes:** numeric.\n* **total intl calls:** numeric.\n* **total intl charge:** numeric.\n* **number customer service calls:** numeric.\n\n**Target/Label:** Churn: True/False"],"metadata":{}},{"cell_type":"markdown","source":["#Load the data\nWe load data from a file from GitHub in to a Spark DataFrame. Each row is an observed customer, and each column contains attributes of that customer."],"metadata":{}},{"cell_type":"code","source":["%sh\nmkdir -p telco\ncurl 'https://raw.githubusercontent.com/Krishnakali/Big-Data-Project/master/bigml_59c28831336c6604c800002a1.csv' > telco/predict_churn.csv\nls /databricks/driver/telco"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%fs \nls file:/databricks/driver/telco"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["from pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\n \nsqlContext = SQLContext(sc)\nschema = StructType([ \\\n    StructField(\"state\", StringType(), True), \\\n    StructField(\"account length\", DoubleType(), True), \\\n    StructField(\"area code\", StringType(), True), \\\n    StructField(\"phone number\", StringType(), True), \\\n    StructField(\"international plan\", StringType(), True), \\\n    StructField(\"voice mail plan\", StringType(), True), \\\n    StructField(\"number vmail messages\", DoubleType(), True), \\\n    StructField(\"total day minutes\", DoubleType(), True), \\\n    StructField(\"total day calls\", DoubleType(), True), \\\n    StructField(\"total day charge\", DoubleType(), True), \\\n    StructField(\"total eve minutes\", DoubleType(), True), \\\n    StructField(\"total eve calls\", DoubleType(), True), \\\n    StructField(\"total eve charge\", DoubleType(), True), \\\n    StructField(\"total night minutes\", DoubleType(), True), \\\n    StructField(\"total night calls\", DoubleType(), True), \\\n    StructField(\"total night charge\", DoubleType(), True), \\\n    StructField(\"total intl minutes\", DoubleType(), True), \\\n    StructField(\"total intl calls\", DoubleType(), True), \\\n    StructField(\"total intl charge\", DoubleType(), True), \\\n    StructField(\"number customer service calls\", DoubleType(), True), \\\n    StructField(\"churn\", StringType(), True)])\ndf_data = spark.read.csv(path=\"file:/databricks/driver/telco/predict_churn.csv\",header='true', schema=schema)\n\nsqlContext.read \\\n    .format('com.databricks.spark.csv') \\\n    .load('file:/databricks/driver/telco/predict_churn.csv', schema = schema)\ndisplay(df_data)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["# Basic DataFrame operations\nDataframes essentially allow us to express sql-like statements."],"metadata":{}},{"cell_type":"code","source":["df_data.count()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["df_data.select('churn').distinct().show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["# Feature Visualization"],"metadata":{}},{"cell_type":"code","source":["df_data.printSchema()\ncols = df_data.columns"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["![DLR](https://raw.githubusercontent.com/Krishnakali/Big-Data-Project/master/Capture.PNG)\n\n\n\n\n\nThe type of visualization we do depends on the data type, so lets define what columns have different properties first:"],"metadata":{}},{"cell_type":"code","source":["numeric_cols = [\"account_length\", \"number_vmail_messages\", \"total_day_minutes\",\n                \"total_day_calls\", \"total_day_charge\", \"total_eve_minutes\",\n                \"total_eve_calls\", \"total_eve_charge\", \"total_night_minutes\",\n                \"total_night_calls\", \"total_intl_minutes\", \"total_intl_calls\",\n                \"total_intl_charge\"]\n\ncategorical_cols = [\"state\", \"international_plan\", \"voice_mail_plan\", \"area_code\"]"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["###Data visualization with SparkR using library ggplot2 and ggally"],"metadata":{}},{"cell_type":"markdown","source":["####Create Dataframe with SparkR"],"metadata":{}},{"cell_type":"code","source":["%r\nlibrary(SparkR)\nchurn <- read.df(\"file:/databricks/driver/telco/predict_churn.csv\",source = \"csv\", header=\"true\", inferSchema = \"true\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["####Create Another Dataframe"],"metadata":{}},{"cell_type":"code","source":["%r\ndf <- collect(select(churn, churn$state, churn$account_length, churn$area_code, churn$phone_number, churn$international_plan, churn$voice_mail_plan, churn$number_vmail_messages, churn$total_day_minutes, churn$total_day_calls, churn$total_day_charge, churn$total_eve_minutes, churn$total_eve_calls, churn$total_eve_charge, churn$total_night_minutes, churn$total_night_calls, churn$total_night_charge, churn$total_intl_minutes, churn$total_intl_calls, churn$total_intl_charge, churn$customer_service_calls, churn$churn))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["####Display Dataframe"],"metadata":{}},{"cell_type":"code","source":["%r\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["####Create Histogram\n* Histogram for the column **total_night_minutes**\n* We see that maximum number of night minutes is above 200."],"metadata":{}},{"cell_type":"code","source":["%r\nhist(df$total_night_minutes,\nxlab = \"total_night_minutes\",\nmain = \"\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["####Create Histogram with Density Line\n* Histogram for the column **total_night_minutes**\n* We see that maximum number of voice mail messages is above 200."],"metadata":{}},{"cell_type":"code","source":["%r\nhist(df$total_night_minutes, prob = TRUE, main = \"\")\nlines(density(df$total_night_minutes))\n\n"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["####Create Histogram with Color"],"metadata":{}},{"cell_type":"code","source":["%r\nhist(df$total_night_minutes, xlab = \"total_night_minutes\",\nmain = \"\", col = \"lightblue\", breaks = 15)\nlines(density(df$total_night_minutes))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["####Create Boxplot using Two Variables\n* Boxplot for column **customer_service_calls** and **churn**\n* Average number of customer service calls of customer likely to retain is 1\n* Average number of customer service calls of customer likely to churn is 2"],"metadata":{}},{"cell_type":"code","source":["%r\nboxplot(customer_service_calls~churn, data = df,  varwidth=TRUE, col = \"lightyellow\")"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["#### Create ggpairs using library GGally\n##### Produces a matrix of scatter plots for visualizing the correlation between total_intl_minutes, total_intl_calls, total_intl_charge."],"metadata":{}},{"cell_type":"code","source":["%r\nrequire(GGally)\ndata = df[, c(17,18,19)]\nggpairs(data=data, # data.frame with variables\n        title=\"International calls data\",\n        upper = list(continuous = \"density\"),\n        lower = list(combo = \"facetdensity\")) # "],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["#### Create HeatMap"],"metadata":{}},{"cell_type":"code","source":["%r\ndata_matrix <- df[, 5:19]\nheat_map <- heatmap(data.matrix(data_matrix), Rowv=NA, Colv=NA, col= cm.colors(256),scale=\"column\", margins=c(25,10))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["# Process Data\nFor analysis we need to convert the categorical variables in the dataset into numeric variables. There are 2 ways we can do this.\n\n* **Category Indexing.**\nThis is basically assigning a numeric value to each category from {0, 1, 2, ...numCategories-1}. This introduces an implicit ordering among your categories, and is more suitable for ordinal variables (eg: Poor: 0, Average: 1, Good: 2)\n\n* **One-Hot Encoding.**\nThis converts categories into binary vectors with at most one nonzero value (eg: (Blue: [1, 0]), (Green: [0, 1]), (Red: [0, 0]))\n\nHere, we will use a combination of StringIndexer and OneHotEncoder to convert the categorical variables. The OneHotEncoder will return a SparseVector.\n\nSince we will have more than 1 stages of feature transformations, we use a Pipeline to tie the stages together. This simplifies our code."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n\ncategoricalColumns = [\"state\", \"area code\", \"phone number\", \"international plan\", \"voice mail plan\"]\nstages = [] \nfor categoricalCol in categoricalColumns:\n\n  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n\n  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n\n  stages += [stringIndexer, encoder]"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["label_stringIdx = StringIndexer(inputCol = \"churn\", outputCol = \"label\")\nstages += [label_stringIdx]"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["numericCols = [\"account length\", \"number vmail messages\", \"total day calls\",\n                        \"total day charge\", \"total eve calls\", \"total eve charge\",\n                        \"total night calls\", \"total intl calls\", \"total intl charge\"]\nassemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["# Model Training\nWe can now define our classifier and pipeline. With this done, we can split our labeled data in train and test sets and fit a model."],"metadata":{}},{"cell_type":"code","source":["pipeline = Pipeline(stages=stages)\n\nmy_Model = pipeline.fit(df_data)\ndf = my_Model.transform(df_data)\n\nselectedcols = [\"label\", \"features\"] + cols\ndf = df.select(selectedcols)\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["splitted_data = df.randomSplit([0.8, 0.2], 24)# proportions [], seed for random\ntrain_data = splitted_data[0]\ntest_data = splitted_data[1]\n\nprint \"Number of training records: \" + str(train_data.count())\nprint \"Number of testing records : \" + str(test_data.count())\n"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["# Logistic Regression\nIn the Pipelines API, we are now able to perform Elastic-Net Regularization with Logistic Regression, as well as other linear methods."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(train_data)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["Make predictions on test data using the transform() method."],"metadata":{}},{"cell_type":"code","source":["predictions = lrModel.transform(test_data)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["Print the schema of the predictions table"],"metadata":{}},{"cell_type":"code","source":["predictions.printSchema()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["We can make use of the BinaryClassificationEvaluator method to evaluate our model. The Evaluator expects two input columns: (rawPrediction, label)."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["Create ParamGrid for Cross Validation"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["#####Create 5-fold CrossValidator and run cross validation"],"metadata":{}},{"cell_type":"code","source":["cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\ncvModel = cv.fit(train_data)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["Use test set here so we can measure the accuracy of our model on new data"],"metadata":{}},{"cell_type":"code","source":["predictions = cvModel.transform(test_data)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["Evaluate the best model"],"metadata":{}},{"cell_type":"code","source":["evaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["We can also access the model’s feature weights and intercepts easily"],"metadata":{}},{"cell_type":"code","source":["print 'Model Intercept: ', cvModel.bestModel.intercept"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["Determine the accuracy of the predictions"],"metadata":{}},{"cell_type":"code","source":["accuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\nprint(\"Accuracy= %g\" % (accuracy))"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["# Random Forest Classifier\nRandom Forests uses an ensemble of trees to improve model accuracy."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\n\n# Create an initial RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n\n# Train model with Training Data\nrfModel = rf.fit(train_data)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":["Make predictions on test data using the Transformer.transform() method."],"metadata":{}},{"cell_type":"code","source":["predictions = rfModel.transform(test_data)"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["Print the schema of the predictions table"],"metadata":{}},{"cell_type":"code","source":["predictions.printSchema()"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["We will evaluate our Random Forest model with BinaryClassificationEvaluator."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["Create ParamGrid for Cross Validation"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(rf.maxDepth, [2, 4, 6])\n             .addGrid(rf.maxBins, [20, 60])\n             .addGrid(rf.numTrees, [5, 20])\n             .build())"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["Create 5-fold CrossValidator and run cross validation"],"metadata":{}},{"cell_type":"code","source":["cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\ncvModel = cv.fit(train_data)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["Use test set here so we can measure the accuracy of our model on new data"],"metadata":{}},{"cell_type":"code","source":["predictions = cvModel.transform(test_data)"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":["Evaluate the best model"],"metadata":{}},{"cell_type":"code","source":["evaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":["# Results\n* Accuracy using Logistic Regression = 78.795%\n* Accuracy using Random Forest Classifier = 84.068%"],"metadata":{}},{"cell_type":"code","source":["truePositive = int(predictions.where(\"(label = 1 AND prediction = 1)\").count())\ntrueNegative = int(predictions.where(\"(label = 0 AND prediction = 0)\").count())\nfalsePositive = int(predictions.where(\"(label = 0 AND prediction = 1)\").count())\nfalseNegative = int(predictions.where(\"(label = 1 AND prediction = 0)\").count())\n\nprint [['TP', truePositive], ['TN', trueNegative], ['FP', falsePositive], ['FN', falseNegative]]\nresultDF = sqlContext.createDataFrame([['TP', truePositive], ['TN', trueNegative], ['FP', falsePositive], ['FN', falseNegative]], ['metric', 'value'])\ndisplay(resultDF)"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["resultDF.createOrReplaceTempView(\"LRresult\")"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["%r\nlibrary(SparkR)\nsparkdf <- sql(\"FROM LRresult SELECT *\")\nrdf <- collect(sparkdf)\nprint( rdf)\nvals <- (t(rdf[2]))\nlabels <- (t(rdf[1]))\n# Simple Pie Chart\npie(vals,labels)"],"metadata":{},"outputs":[],"execution_count":78}],"metadata":{"name":"Predict Customer Churn_Final","notebookId":4251293995586136},"nbformat":4,"nbformat_minor":0}
